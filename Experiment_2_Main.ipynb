{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This experiment creates a toy model (Features being: xyz) and trains it on 2 narrow classes of data. "
      ],
      "metadata": {
        "id": "ynCni3sDunUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#class_names = [\"Dogs\", \"Cars\"]\n",
        "\n",
        "#num_classes = len(class_names)\n",
        "#num_features = \n",
        "# Define the number of input features and output classes\n",
        "num_features = 10000\n",
        "num_classes = 2\n",
        "hidden_neurons = 100\n",
        "semantic_threshold = .6\n",
        "\n",
        "# Generate random training data\n",
        "X_train = torch.Tensor(np.random.rand(100, num_features))\n",
        "y_train = torch.Tensor(np.random.randint(num_classes, size=100))"
      ],
      "metadata": {
        "id": "edW6Usk9rUOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test for cosine similarity approach of polysemanticity (Helped by Mika Kimmins)\n",
        "def is_polysemantic(neuron, other_neurons, threshold):\n",
        "    # Compute the weight vector for the neuron.\n",
        "    w_neuron = neuron.view(-1)\n",
        "\n",
        "    # Compute the weight vectors for the other neurons.\n",
        "    w_others = torch.stack([other.view(-1) for other in other_neurons])\n",
        "\n",
        "    # Compute the cosine similarity between the weight vector for the neuron and the weight vectors for the other neurons.\n",
        "    cos_sim = F.cosine_similarity(w_neuron.unsqueeze(0), w_others, dim=1)\n",
        "\n",
        "    # Determine if the neuron is polysemantic or monosemantic based on the cosine similarities.\n",
        "    return (cos_sim > threshold).any().item()\n",
        "\n",
        "# Returns how polysemantic a given neuron is\n",
        "def how_polysemantic(neuron, other_neurons):\n",
        "  w_neuron = neuron.view(-1)\n",
        "  w_others = torch.stack([other.view(-1) for other in other_neurons])\n",
        "  cos_sim = F.cosine_similarity(w_neuron.unsqueeze(0), w_others, dim=1)\n",
        "  return max(cos_sim)\n",
        "\n",
        "# Define a neural network model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_features, hidden_neurons)\n",
        "        self.fc2 = nn.Linear(hidden_neurons, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.softmax(self.fc2(x), dim=1)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "kRwejXofuVG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train.long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "6gPT9LW4ucpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the is_polysemantic function to analyze the model's neurons\n",
        "for name, param in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        for i, neuron in enumerate(param):\n",
        "            how_poly = how_polysemantic(neuron, torch.cat([param[0:i], param[i+1:]]))\n",
        "            print(f'{how_poly}')\n",
        "            #is_poly = is_polysemantic(neuron, torch.cat([param[0:i], param[i+1:]]), semantic_threshold)\n",
        "            #if is_poly:\n",
        "            #    print(f'Neuron {i} in {name} is polysemantic')\n",
        "            #else:\n",
        "            #    print(f'Neuron {i} in {name} is monosemantic')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtaks4akufVl",
        "outputId": "dd78ad27-5a30-4050-a72f-306989b70105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3421878218650818\n",
            "0.308621346950531\n",
            "0.1502845585346222\n",
            "0.13724738359451294\n",
            "0.18378958106040955\n",
            "0.16899050772190094\n",
            "0.10175445675849915\n",
            "0.3424777686595917\n",
            "0.3680996596813202\n",
            "0.1910419464111328\n",
            "0.33347848057746887\n",
            "0.15837961435317993\n",
            "0.2958216071128845\n",
            "0.18699130415916443\n",
            "0.17130568623542786\n",
            "0.31885623931884766\n",
            "0.11336076259613037\n",
            "0.28934037685394287\n",
            "0.19396856427192688\n",
            "0.21569108963012695\n",
            "0.3263435363769531\n",
            "0.29732102155685425\n",
            "0.15565745532512665\n",
            "0.28293728828430176\n",
            "0.3301575481891632\n",
            "0.2961098551750183\n",
            "0.28892579674720764\n",
            "0.33416032791137695\n",
            "0.3398621082305908\n",
            "0.22609740495681763\n",
            "0.3285053074359894\n",
            "0.14037668704986572\n",
            "0.3539523482322693\n",
            "0.3184264898300171\n",
            "0.20265741646289825\n",
            "0.3128518760204315\n",
            "0.33099883794784546\n",
            "0.025247933343052864\n",
            "0.024417703971266747\n",
            "0.24185055494308472\n",
            "0.019430601969361305\n",
            "0.12254232168197632\n",
            "0.24378640949726105\n",
            "0.1939958781003952\n",
            "0.32554376125335693\n",
            "0.30025264620780945\n",
            "0.350033700466156\n",
            "0.16195282340049744\n",
            "0.22487430274486542\n",
            "0.28934037685394287\n",
            "0.15472851693630219\n",
            "0.3191687762737274\n",
            "0.21464958786964417\n",
            "0.34066158533096313\n",
            "0.20743030309677124\n",
            "0.25299859046936035\n",
            "0.18667413294315338\n",
            "0.2961098551750183\n",
            "0.14865291118621826\n",
            "0.19766643643379211\n",
            "0.15062063932418823\n",
            "0.1581239402294159\n",
            "0.15546780824661255\n",
            "0.21494625508785248\n",
            "0.3123480975627899\n",
            "0.25776350498199463\n",
            "0.3539523482322693\n",
            "0.32450762391090393\n",
            "0.26120755076408386\n",
            "0.14117363095283508\n",
            "0.1761954128742218\n",
            "0.17138326168060303\n",
            "0.24185055494308472\n",
            "0.33099883794784546\n",
            "0.21726074814796448\n",
            "0.1782807558774948\n",
            "0.28830042481422424\n",
            "0.35234010219573975\n",
            "0.2603655457496643\n",
            "0.28728875517845154\n",
            "0.1743163764476776\n",
            "0.10603281110525131\n",
            "0.3421878218650818\n",
            "0.2462570071220398\n",
            "0.3290365934371948\n",
            "0.2559792995452881\n",
            "0.28602758049964905\n",
            "0.3644413948059082\n",
            "0.22609740495681763\n",
            "0.3373050391674042\n",
            "0.1946692168712616\n",
            "0.32328033447265625\n",
            "0.33416032791137695\n",
            "0.23918011784553528\n",
            "0.28506937623023987\n",
            "0.2416333705186844\n",
            "0.2494969367980957\n",
            "0.021495351567864418\n",
            "0.024125035852193832\n",
            "0.3680996596813202\n",
            "0.07498615980148315\n",
            "0.07498615980148315\n"
          ]
        }
      ]
    }
  ]
}